% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.


@ARTICLE{Garyfallidis2014-el,
  title       = "Dipy, a library for the analysis of diffusion {MRI} data",
  author      = "Garyfallidis, Eleftherios and Brett, Matthew and Amirbekian,
                 Bagrat and Rokem, Ariel and van der Walt, Stefan and
                 Descoteaux, Maxime and Nimmo-Smith, Ian and {Dipy
                 Contributors}",
  affiliation = "Computer Science Department, University of Sherbrooke
                 Sherbrooke, QC, Canada ; MRC Cognition and Brain Sciences
                 Unit, University of Cambridge Cambridge, UK. Henry H. Wheeler,
                 Jr. Brain Imaging Center, University of California Berkeley,
                 CA, USA. Department of Neurology and Graduate Group in
                 Bioengineering, University of California San Francisco, CA,
                 USA. Department of Psychology, Stanford University Stanford,
                 CA, USA. Department of Mathematical Sciences, Division of
                 Applied Mathematics, Stellenbosch University Stellenbosch,
                 South Africa. MRC Cognition and Brain Sciences Unit,
                 University of Cambridge Cambridge, UK. MRC Cognition and Brain
                 Sciences Unit, University of Cambridge Cambridge, UK.",
  abstract    = "Diffusion Imaging in Python (Dipy) is a free and open source
                 software project for the analysis of data from diffusion
                 magnetic resonance imaging (dMRI) experiments. dMRI is an
                 application of MRI that can be used to measure structural
                 features of brain white matter. Many methods have been
                 developed to use dMRI data to model the local configuration of
                 white matter nerve fiber bundles and infer the trajectory of
                 bundles connecting different parts of the brain. Dipy gathers
                 implementations of many different methods in dMRI, including:
                 diffusion signal pre-processing; reconstruction of diffusion
                 distributions in individual voxels; fiber tractography and
                 fiber track post-processing, analysis and visualization. Dipy
                 aims to provide transparent implementations for all the
                 different steps of dMRI analysis with a uniform programming
                 interface. We have implemented classical signal reconstruction
                 techniques, such as the diffusion tensor model and
                 deterministic fiber tractography. In addition, cutting edge
                 novel reconstruction techniques are implemented, such as
                 constrained spherical deconvolution and diffusion spectrum
                 imaging (DSI) with deconvolution, as well as methods for
                 probabilistic tracking and original methods for tractography
                 clustering. Many additional utility functions are provided to
                 calculate various statistics, informative visualizations, as
                 well as file-handling routines to assist in the development
                 and use of novel techniques. In contrast to many other
                 scientific software projects, Dipy is not being developed by a
                 single research group. Rather, it is an open project that
                 encourages contributions from any scientist/developer through
                 GitHub and open discussions on the project mailing list.
                 Consequently, Dipy today has an international team of
                 contributors, spanning seven different academic institutions
                 in five countries and three continents, which is still
                 growing.",
  journal     = "Front. Neuroinform.",
  volume      =  8,
  pages       = "8",
  month       =  feb,
  year        =  2014,
  keywords    = "DSI; DTI; HARDI; Python; dMRI; diffusion MRI; free open source
                 software; tractography"
}

@ARTICLE{Simonyan2014-ua,
  title         = "Very Deep Convolutional Networks for {Large-Scale} Image
                   Recognition",
  author        = "Simonyan, Karen and Zisserman, Andrew",
  abstract      = "In this work we investigate the effect of the convolutional
                   network depth on its accuracy in the large-scale image
                   recognition setting. Our main contribution is a thorough
                   evaluation of networks of increasing depth using an
                   architecture with very small (3x3) convolution filters,
                   which shows that a significant improvement on the prior-art
                   configurations can be achieved by pushing the depth to 16-19
                   weight layers. These findings were the basis of our ImageNet
                   Challenge 2014 submission, where our team secured the first
                   and the second places in the localisation and classification
                   tracks respectively. We also show that our representations
                   generalise well to other datasets, where they achieve
                   state-of-the-art results. We have made our two
                   best-performing ConvNet models publicly available to
                   facilitate further research on the use of deep visual
                   representations in computer vision.",
  month         =  sep,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1409.1556"
}

@ARTICLE{Avants2008-sa,
  title       = "Symmetric diffeomorphic image registration with
                 cross-correlation: evaluating automated labeling of elderly
                 and neurodegenerative brain",
  author      = "Avants, B B and Epstein, C L and Grossman, M and Gee, J C",
  affiliation = "Department of Radiology, University of Pennsylvania, 3600
                 Market Street, Philadelphia, PA 19104, United States.
                 avants@grasp.cis.upenn.edu",
  abstract    = "One of the most challenging problems in modern neuroimaging is
                 detailed characterization of neurodegeneration. Quantifying
                 spatial and longitudinal atrophy patterns is an important
                 component of this process. These spatiotemporal signals will
                 aid in discriminating between related diseases, such as
                 frontotemporal dementia (FTD) and Alzheimer's disease (AD),
                 which manifest themselves in the same at-risk population.
                 Here, we develop a novel symmetric image normalization method
                 (SyN) for maximizing the cross-correlation within the space of
                 diffeomorphic maps and provide the Euler-Lagrange equations
                 necessary for this optimization. We then turn to a careful
                 evaluation of our method. Our evaluation uses gold standard,
                 human cortical segmentation to contrast SyN's performance with
                 a related elastic method and with the standard ITK
                 implementation of Thirion's Demons algorithm. The new method
                 compares favorably with both approaches, in particular when
                 the distance between the template brain and the target brain
                 is large. We then report the correlation of volumes gained by
                 algorithmic cortical labelings of FTD and control subjects
                 with those gained by the manual rater. This comparison shows
                 that, of the three methods tested, SyN's volume measurements
                 are the most strongly correlated with volume measurements
                 gained by expert labeling. This study indicates that SyN, with
                 cross-correlation, is a reliable method for normalizing and
                 making anatomical measurements in volumetric MRI of patients
                 and at-risk elderly individuals.",
  journal     = "Med. Image Anal.",
  publisher   = "Elsevier",
  volume      =  12,
  number      =  1,
  pages       = "26--41",
  month       =  feb,
  year        =  2008
}


@INPROCEEDINGS{Johnson2016-ac,
  title      = "Perceptual Losses for {Real-Time} Style Transfer and
                {Super-Resolution}",
  booktitle  = "Computer Vision -- {ECCV} 2016",
  author     = "Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li",
  abstract   = "We consider image transformation problems, where an input image
                is transformed into an output image. Recent methods for such
                problems typically train feed-forward convolutional neural
                networks using a per-pixel loss between the output and
                ground-truth images. Parallel work has shown that high-quality
                images can be generated by defining and optimizing perceptual
                loss functions based on high-level features extracted from
                pretrained networks. We combine the benefits of both
                approaches, and propose the use of perceptual loss functions
                for training feed-forward networks for image transformation
                tasks. We show results on image style transfer, where a
                feed-forward network is trained to solve the optimization
                problem proposed by Gatys et al. in real-time. Compared to the
                optimization-based method, our network gives similar
                qualitative results but is three orders of magnitude faster. We
                also experiment with single-image super-resolution, where
                replacing a per-pixel loss with a perceptual loss gives
                visually pleasing results.",
  publisher  = "Springer, Cham",
  pages      = "694--711",
  series     = "Lecture Notes in Computer Science",
  month      =  oct,
  year       =  2016,
  language   = "en",
  conference = "European Conference on Computer Vision"
}

@ARTICLE{Klein2009-gq,
  title       = "Evaluation of 14 nonlinear deformation algorithms applied to
                 human brain {MRI} registration",
  author      = "Klein, Arno and Andersson, Jesper and Ardekani, Babak A and
                 Ashburner, John and Avants, Brian and Chiang, Ming-Chang and
                 Christensen, Gary E and Collins, D Louis and Gee, James and
                 Hellier, Pierre and Song, Joo Hyun and Jenkinson, Mark and
                 Lepage, Claude and Rueckert, Daniel and Thompson, Paul and
                 Vercauteren, Tom and Woods, Roger P and Mann, J John and
                 Parsey, Ramin V",
  affiliation = "New York State Psychiatric Institute, Columbia University, NY,
                 NY 10032, USA. arno@binarybottle.com",
  abstract    = "All fields of neuroscience that employ brain imaging need to
                 communicate their results with reference to anatomical
                 regions. In particular, comparative morphometry and group
                 analysis of functional and physiological data require
                 coregistration of brains to establish correspondences across
                 brain structures. It is well established that linear
                 registration of one brain to another is inadequate for
                 aligning brain structures, so numerous algorithms have emerged
                 to nonlinearly register brains to one another. This study is
                 the largest evaluation of nonlinear deformation algorithms
                 applied to brain image registration ever conducted. Fourteen
                 algorithms from laboratories around the world are evaluated
                 using 8 different error measures. More than 45,000
                 registrations between 80 manually labeled brains were
                 performed by algorithms including: AIR, ANIMAL, ART,
                 Diffeomorphic Demons, FNIRT, IRTK, JRD-fluid, ROMEO, SICLE,
                 SyN, and four different SPM5 algorithms (``SPM2-type'' and
                 regular Normalization, Unified Segmentation, and the DARTEL
                 Toolbox). All of these registrations were preceded by linear
                 registration between the same image pairs using FLIRT. One of
                 the most significant findings of this study is that the
                 relative performances of the registration methods under
                 comparison appear to be little affected by the choice of
                 subject population, labeling protocol, and type of overlap
                 measure. This is important because it suggests that the
                 findings are generalizable to new subject populations that are
                 labeled or evaluated using different labeling protocols.
                 Furthermore, we ranked the 14 methods according to three
                 completely independent analyses (permutation tests, one-way
                 ANOVA tests, and indifference-zone ranking) and derived three
                 almost identical top rankings of the methods. ART, SyN, IRTK,
                 and SPM's DARTEL Toolbox gave the best results according to
                 overlap and distance measures, with ART and SyN delivering the
                 most consistently high accuracy across subjects and label
                 sets. Updates will be published on the
                 http://www.mindboggle.info/papers/ website.",
  journal     = "Neuroimage",
  volume      =  46,
  number      =  3,
  pages       = "786--802",
  month       =  jul,
  year        =  2009,
  language    = "en"
}

@ARTICLE{Vymazal1995-zo,
  title       = "The quantitative relation between T1-weighted and T2-weighted
                 {MRI} of normal gray matter and iron concentration",
  author      = "Vymazal, J and Hajek, M and Patronas, N and Giedd, J N and
                 Bulte, J W and Baumgarner, C and Tran, V and Brooks, R A",
  affiliation = "Neuroimaging Branch, National Institute of Neurological
                 Disorders and Stroke, National Institutes of Health, Bethesda,
                 MD 20892, USA.",
  abstract    = "A retrospective analysis of 158 T1-weighted and T2-weighted
                 MRI scans of normal brains at 0.5 and 1.5 Tesla was performed.
                 Signal intensities in the frontal cortex, caudate nucleus,
                 putamen, and globus pallidus were divided by those of frontal
                 white matter; and these gray/white ratios were correlated with
                 iron concentration, estimated from the anatomical region and
                 age of the patient. Intraregional plots were also made of
                 gray/white ratio versus age for the 1.5 Tesla scans. The
                 changes in both T1-weighted and T2-weighted ratios were
                 consistent with the hypothesis that 1/T1 and 1/T2 vary
                 linearly with iron concentration, and the corresponding
                 coefficients, determined separately from the interregional and
                 intraregional plots, were generally in agreement. Furthermore,
                 the variability of the MRI ratios at 1.5 Tesla was consistent
                 with expected iron variability except for the cortex, in which
                 partial volume errors due to sulci and white matter caused
                 increased variations. The MRI results agreed well with in
                 vitro data on T1 and T2 in tissue specimens and with other MRI
                 studies. When compared with T1 and T2 in ferritin solution, a
                 significant ``tissue relaxation enhancement'' was found,
                 attributable to slower diffusion and clustering of ferritin in
                 tissue.",
  journal     = "J. Magn. Reson. Imaging",
  volume      =  5,
  number      =  5,
  pages       = "554--560",
  month       =  sep,
  year        =  1995,
  language    = "en"
}


@ARTICLE{Ronneberger2015-ua,
  title         = "{U-Net}: Convolutional Networks for Biomedical Image
                   Segmentation",
  author        = "Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",
  abstract      = "There is large consent that successful training of deep
                   networks requires many thousand annotated training samples.
                   In this paper, we present a network and training strategy
                   that relies on the strong use of data augmentation to use
                   the available annotated samples more efficiently. The
                   architecture consists of a contracting path to capture
                   context and a symmetric expanding path that enables precise
                   localization. We show that such a network can be trained
                   end-to-end from very few images and outperforms the prior
                   best method (a sliding-window convolutional network) on the
                   ISBI challenge for segmentation of neuronal structures in
                   electron microscopic stacks. Using the same network trained
                   on transmitted light microscopy images (phase contrast and
                   DIC) we won the ISBI cell tracking challenge 2015 in these
                   categories by a large margin. Moreover, the network is fast.
                   Segmentation of a 512x512 image takes less than a second on
                   a recent GPU. The full implementation (based on Caffe) and
                   the trained networks are available at
                   http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net
                   .",
  month         =  may,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1505.04597"
}



@ARTICLE{Isola2016-oc,
  title         = "{Image-to-Image} Translation with Conditional Adversarial
                   Networks",
  author        = "Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros,
                   Alexei A",
  abstract      = "We investigate conditional adversarial networks as a
                   general-purpose solution to image-to-image translation
                   problems. These networks not only learn the mapping from
                   input image to output image, but also learn a loss function
                   to train this mapping. This makes it possible to apply the
                   same generic approach to problems that traditionally would
                   require very different loss formulations. We demonstrate
                   that this approach is effective at synthesizing photos from
                   label maps, reconstructing objects from edge maps, and
                   colorizing images, among other tasks. Indeed, since the
                   release of the pix2pix software associated with this paper,
                   a large number of internet users (many of them artists) have
                   posted their own experiments with our system, further
                   demonstrating its wide applicability and ease of adoption
                   without the need for parameter tweaking. As a community, we
                   no longer hand-engineer our mapping functions, and this work
                   suggests we can achieve reasonable results without
                   hand-engineering our loss functions either.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1611.07004"
}
